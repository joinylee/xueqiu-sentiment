#!/usr/bin/env python3
"""
é›ªçƒèˆ†æƒ…ç›‘æ§ - V3 æœ€ç»ˆç‰ˆ
åŸºäº GitHub å¼€æºæ–¹æ¡ˆä¼˜åŒ–
"""

import requests
import json
import re
import time
import os
from datetime import datetime
from typing import List, Dict, Optional

# ============ é…ç½® ============
SYMBOLS = [
    ("SH600118", "ä¸­å›½å«æ˜Ÿ"),
    ("SZ002155", "æ¹–å—é»„é‡‘"),
    ("SZ300456", "èµ›å¾®ç”µå­"),
    ("SH600879", "èˆªå¤©ç”µå­"),
    ("SZ002565", "é¡ºçè‚¡ä»½"),
]

OUTPUT_DIR = "/Users/joinylee/Openclaw/xueqiu_sentiment/reports"
MAX_PAGES = 3  # æ¯åªè‚¡ç¥¨æœ€å¤§é¡µæ•°
COOKIES = {
    "xq_a_token": "601797f192b2540dd1885fc7d1cddc7b48374a0b",
    "u": "2274226566",
}

# ============ æƒ…ç»ªåˆ†æ ============
def analyze_sentiment(text: str) -> str:
    """ç®€å•æƒ…ç»ªåˆ†æ"""
    bullish = ['æ¶¨', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'çªç ´', 'å¼ºåŠ¿', 'æ–°é«˜', 'åšå¤š', 'æŠ„åº•', 'æ‹‰å‡', 'åå¼¹']
    bearish = ['è·Œ', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'ç ´ä½', 'å¼±åŠ¿', 'æ–°ä½', 'åšç©º', 'å‰²è‚‰', 'æ±ªæ±ª', 'å‰²äº†', 'æ‰“å‹']
    
    text = text.lower()
    bull = sum(1 for w in bullish if w in text)
    bear = sum(1 for w in bearish if w in text)
    
    if bull > bear:
        return "ğŸŸ¢ åˆ©å¤š"
    elif bear > bull:
        return "ğŸ”´ åˆ©ç©º"
    return "âšª ä¸­æ€§"

# ============ æ ¸å¿ƒæŠ“å–å‡½æ•° ============
def fetch_posts(symbol: str, page: int = 1) -> List[Dict]:
    """
    æŠ“å–é›ªçƒè®¨è®º
    API: https://xueqiu.com/statuses/search.json
    """
    timestamp = int(time.time() * 1000)
    
    url = f"https://xueqiu.com/statuses/search.json"
    params = {
        "count": 20,
        "comment": 0,
        "symbol": symbol,
        "hl": 0,
        "source": "user",
        "sort": "time",
        "page": page,
        "_": timestamp,
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": f"https://xueqiu.com/S/{symbol}",
        "Accept": "application/json, text/javascript, */*; q=0.01",
        "X-Requested-With": "XMLHttpRequest",
    }
    
    try:
        session = requests.Session()
        
        # å…ˆè®¿é—®ä¸»é¡µè·å– cookie
        session.get("https://xueqiu.com/", headers=headers, timeout=10)
        
        # è®¾ç½®å·²çŸ¥ cookie
        for key, value in COOKIES.items():
            session.cookies.set(key, value)
        
        # è¯·æ±‚ API
        resp = session.get(url, params=params, headers=headers, timeout=15)
        
        if resp.status_code == 200:
            try:
                data = resp.json()
                return data.get("list", [])
            except json.JSONDecodeError:
                print(f"   âš ï¸ JSONè§£æå¤±è´¥")
                return []
        elif resp.status_code == 403:
            print(f"   âš ï¸ 403 éœ€è¦éªŒè¯")
            return []
        else:
            print(f"   âš ï¸ HTTP {resp.status_code}")
            return []
            
    except Exception as e:
        print(f"   âŒ è¯·æ±‚å¤±è´¥: {e}")
        return []

def fetch_stock_discussions(symbol: str, name: str) -> List[Dict]:
    """æŠ“å–å•åªè‚¡ç¥¨24å°æ—¶å†…çš„è®¨è®º"""
    print(f"\nğŸ“¡ {name} ({symbol})")
    print("-" * 60)
    
    now_ts = datetime.now().timestamp() * 1000
    one_day_ms = 24 * 60 * 60 * 1000
    
    all_posts = []
    
    for page in range(1, MAX_PAGES + 1):
        print(f"   æŠ“å–ç¬¬ {page} é¡µ...", end=" ")
        
        posts = fetch_posts(symbol, page)
        if not posts:
            print("æ— æ•°æ®")
            break
        
        valid_count = 0
        stop_fetch = False
        
        for post in posts:
            ts = post.get("created_at", 0)
            
            # æ£€æŸ¥24å°æ—¶
            if now_ts - ts > one_day_ms:
                stop_fetch = True
                break
            
            # æ¸…æ´—æ–‡æœ¬
            text = re.sub(r'<[^>]+>', '', post.get("text", ""))
            text = text.replace("&nbsp;", " ").replace("&quot;", '"').strip()
            
            if len(text) < 5:
                continue
            
            all_posts.append({
                "text": text,
                "author": post.get("user", {}).get("screen_name", "åŒ¿å"),
                "time": datetime.fromtimestamp(ts/1000).strftime("%m-%d %H:%M"),
                "sentiment": analyze_sentiment(text),
                "likes": post.get("like_count", 0),
                "comments": post.get("reply_count", 0),
            })
            valid_count += 1
        
        print(f"{valid_count} æ¡")
        
        if stop_fetch:
            print(f"   â° è¶…å‡º24å°æ—¶ï¼Œåœæ­¢")
            break
        
        time.sleep(1)  # é™é€Ÿ
    
    print(f"   âœ… æ€»è®¡: {len(all_posts)} æ¡")
    return all_posts

# ============ ç”ŸæˆæŠ¥å‘Š ============
def generate_markdown(all_data: Dict) -> str:
    """ç”Ÿæˆ Markdown æŠ¥å‘Š"""
    now = datetime.now()
    report = f"""# ğŸ“Š é›ªçƒèˆ†æƒ…ç›‘æ§æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {now.strftime('%Y-%m-%d %H:%M:%S')}
**ç›‘æ§è‚¡ç¥¨**: {len(SYMBOLS)} åª

---

"""
    
    for symbol, name in SYMBOLS:
        posts = all_data.get(symbol, [])
        
        # ç»Ÿè®¡
        bull = len([p for p in posts if "åˆ©å¤š" in p["sentiment"]])
        bear = len([p for p in posts if "åˆ©ç©º" in p["sentiment"]])
        
        report += f"""## ğŸ“ˆ {name} ({symbol})

**ç»Ÿè®¡**: å…± {len(posts)} æ¡ | ğŸŸ¢ {bull} | ğŸ”´ {bear}

"""
        
        if posts:
            report += "### æœ€æ–°è®¨è®º\n\n"
            for i, p in enumerate(posts[:5], 1):
                report += f"{i}. {p['sentiment']} **{p['time']}** | {p['author']}\n"
                report += f"   > {p['text'][:80]}{'...' if len(p['text']) > 80 else ''}\n\n"
        else:
            report += "*æš‚æ— æ•°æ®*\n\n"
        
        report += "---\n\n"
    
    return report

def save_results(all_data: Dict, report: str):
    """ä¿å­˜ç»“æœ"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # JSON
    json_file = os.path.join(OUTPUT_DIR, f'xueqiu_{timestamp}.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({"time": datetime.now().isoformat(), "data": all_data}, f, ensure_ascii=False, indent=2)
    
    # Markdown
    md_file = os.path.join(OUTPUT_DIR, f'report_{timestamp}.md')
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜:")
    print(f"   JSON: {json_file}")
    print(f"   æŠ¥å‘Š: {md_file}")

# ============ ä¸»ç¨‹åº ============
def main():
    print("=" * 60)
    print("ğŸ§ é›ªçƒèˆ†æƒ…ç›‘æ§ - V3 æœ€ç»ˆç‰ˆ")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    all_data = {}
    
    for symbol, name in SYMBOLS:
        posts = fetch_stock_discussions(symbol, name)
        all_data[symbol] = posts
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”ŸæˆæŠ¥å‘Š...")
    print("=" * 60)
    
    report = generate_markdown(all_data)
    save_results(all_data, report)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“ˆ ç›‘æ§æ‘˜è¦:")
    print("-" * 60)
    for symbol, name in SYMBOLS:
        posts = all_data.get(symbol, [])
        bull = len([p for p in posts if "åˆ©å¤š" in p["sentiment"]])
        bear = len([p for p in posts if "åˆ©ç©º" in p["sentiment"]])
        print(f"   {name}: {len(posts)}æ¡ (ğŸŸ¢{bull} ğŸ”´{bear})")
    
    print("\n" + "=" * 60)
    print("âœ… ç›‘æ§å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
