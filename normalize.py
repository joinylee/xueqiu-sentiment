#!/usr/bin/env python3
"""
æ•°æ®æ ‡å‡†åŒ–æ¨¡å—
å°†ä¸åŒæ¥æºçš„é›ªçƒæ•°æ®ç»Ÿä¸€æˆæ ‡å‡†æ ¼å¼
"""

import json
import re
from datetime import datetime
from typing import Dict, List, Optional

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦"""
    if not text:
        return ""
    
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text).strip()
    # ç§»é™¤é›ªçƒç‰¹æœ‰è¡¨æƒ…æ ‡ç­¾
    text = re.sub(r'\[.*?\]', '', text)
    
    return text

def normalize_status(item: Dict, symbol: str) -> Dict:
    """
    æ ‡å‡†åŒ–ä¸ªè‚¡è®¨è®ºæ•°æ®
    
    Args:
        item: åŸå§‹æ•°æ®
        symbol: è‚¡ç¥¨ä»£ç 
    
    Returns:
        dict: æ ‡å‡†åŒ–åçš„æ•°æ®
    """
    user = item.get("user", {})
    
    return {
        "id": str(item.get("id", "")),
        "symbol": symbol,
        "source": "xueqiu",
        "type": "status",
        "author": user.get("screen_name", ""),
        "author_id": user.get("id", ""),
        "text": clean_text(item.get("text", "")),
        "raw_text": item.get("text", ""),  # ä¿ç•™åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
        "likes": item.get("like_count", 0),
        "comments": item.get("comment_count", 0),
        "reposts": item.get("repost_count", 0),
        "created_at": datetime.fromtimestamp(item.get("created_at", 0) / 1000).isoformat(),
        "timestamp": item.get("created_at", 0) // 1000,  # Unixæ—¶é—´æˆ³
        "url": f"https://xueqiu.com/S/{symbol}/{item.get('id', '')}",
    }

def normalize_livenews(item: Dict) -> Dict:
    """
    æ ‡å‡†åŒ–å¿«è®¯æ•°æ®
    
    Args:
        item: åŸå§‹æ•°æ®
    
    Returns:
        dict: æ ‡å‡†åŒ–åçš„æ•°æ®
    """
    return {
        "id": str(item.get("id", "")),
        "symbol": None,  # å¿«è®¯å¯èƒ½ä¸å…³è”ç‰¹å®šè‚¡ç¥¨
        "source": "xueqiu",
        "type": "livenews",
        "author": "é›ªçƒå¿«è®¯",
        "author_id": "system",
        "text": clean_text(item.get("text", "")),
        "raw_text": item.get("text", ""),
        "likes": 0,
        "comments": 0,
        "reposts": 0,
        "created_at": datetime.fromtimestamp(item.get("created_at", 0) / 1000).isoformat(),
        "timestamp": item.get("created_at", 0) // 1000,
        "url": None,
    }

def normalize_all(status_data: List[Dict], livenews_data: List[Dict], symbols: List[str]) -> List[Dict]:
    """
    æ ‡å‡†åŒ–æ‰€æœ‰æ•°æ®
    
    Args:
        status_data: ä¸ªè‚¡è®¨è®ºåŸå§‹æ•°æ®
        livenews_data: å¿«è®¯åŸå§‹æ•°æ®
        symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆç”¨äºå…³è”symbolï¼‰
    
    Returns:
        list: æ ‡å‡†åŒ–åçš„æ•°æ®åˆ—è¡¨
    """
    normalized = []
    
    # å¤„ç†ä¸ªè‚¡è®¨è®º
    for item in status_data:
        # å°è¯•è·å–å…³è”çš„è‚¡ç¥¨ä»£ç 
        symbol = item.get("symbol", "")
        if not symbol:
            # ä»é“¾æ¥ä¸­æå–
            text = item.get("text", "")
            match = re.search(r'(SH|SZ)\d{6}', text)
            if match:
                symbol = match.group(0)
        
        normalized.append(normalize_status(item, symbol))
    
    # å¤„ç†å¿«è®¯
    for item in livenews_data:
        normalized.append(normalize_livenews(item))
    
    # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    normalized.sort(key=lambda x: x.get("timestamp", 0), reverse=True)
    
    return normalized

def load_raw_data(prefix: str = "/tmp/xueqiu") -> tuple:
    """
    åŠ è½½åŸå§‹æ•°æ®
    
    Args:
        prefix: æ–‡ä»¶å‰ç¼€
    
    Returns:
        tuple: (status_data, livenews_data)
    """
    import os
    
    status_file = f"{prefix}_status_raw.json"
    livenews_file = f"{prefix}_livenews_raw.json"
    
    status_data = []
    livenews_data = []
    
    if os.path.exists(status_file):
        with open(status_file, "r", encoding="utf-8") as f:
            status_data = json.load(f)
    
    if os.path.exists(livenews_file):
        with open(livenews_file, "r", encoding="utf-8") as f:
            livenews_data = json.load(f)
    
    return status_data, livenews_data

def save_normalized_data(data: List[Dict], filename: str = "/tmp/xueqiu_normalized.jsonl"):
    """
    ä¿å­˜æ ‡å‡†åŒ–æ•°æ®ï¼ˆJSONLæ ¼å¼ï¼Œæ¯è¡Œä¸€æ¡ï¼‰
    
    Args:
        data: æ ‡å‡†åŒ–æ•°æ®
        filename: è¾“å‡ºæ–‡ä»¶å
    """
    with open(filename, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"ğŸ’¾ å·²ä¿å­˜ {len(data)} æ¡æ ‡å‡†åŒ–æ•°æ®åˆ° {filename}")

if __name__ == "__main__":
    from config import SYMBOLS
    
    print("=" * 60)
    print("ğŸ”§ æ•°æ®æ ‡å‡†åŒ–")
    print("=" * 60)
    
    # åŠ è½½åŸå§‹æ•°æ®
    status_data, livenews_data = load_raw_data()
    
    print(f"ğŸ“¥ åŠ è½½åŸå§‹æ•°æ®:")
    print(f"  - ä¸ªè‚¡è®¨è®º: {len(status_data)} æ¡")
    print(f"  - å¿«è®¯: {len(livenews_data)} æ¡")
    
    if not status_data and not livenews_data:
        print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°åŸå§‹æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ fetch_status.py å’Œ fetch_livenews.py")
    else:
        # æ ‡å‡†åŒ–
        normalized = normalize_all(status_data, livenews_data, SYMBOLS)
        print(f"\nâœ… æ ‡å‡†åŒ–å®Œæˆ: {len(normalized)} æ¡")
        
        # ä¿å­˜
        save_normalized_data(normalized)
