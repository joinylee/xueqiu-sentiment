#!/usr/bin/env python3
"""
é›ªçƒèˆ†æƒ…ç›‘æ§ - å®Œæ•´ç‰ˆ v2.0
åŠŸèƒ½ï¼š
1. 24å°æ—¶æ—¶é—´çª—å£æŠ“å–
2. è‡ªåŠ¨ç¿»é¡µï¼ˆmax_idæœºåˆ¶ï¼‰
3. ä½¿ç”¨browserç»•è¿‡WAF
4. é™é€Ÿé˜²å°
5. æƒ…ç»ªåˆ†æ + å…³é”®è¯æå–
6. ç»“æœä¿å­˜ä¸ºJSONå’ŒMarkdownæŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python xueqiu_monitor_v2.py
"""

import subprocess
import json
import re
import time
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import sys

# ============ é…ç½® ============
SYMBOLS = [
    ("SH600118", "ä¸­å›½å«æ˜Ÿ"),
    ("SZ002155", "æ¹–å—é»„é‡‘"),
    ("SZ300456", "èµ›å¾®ç”µå­"),
    ("SH600879", "èˆªå¤©ç”µå­"),
    ("SZ002565", "é¡ºçè‚¡ä»½"),
]

OUTPUT_DIR = "/Users/joinylee/Openclaw/xueqiu_sentiment/reports"
MAX_PAGES = 5  # æ¯åªè‚¡ç¥¨æœ€å¤§ç¿»é¡µæ•°
SLEEP_TIME = 1.5  # ç¿»é¡µé—´éš”ï¼ˆç§’ï¼‰

# ============ æƒ…ç»ªåˆ†æ ============
BULLISH_WORDS = ['æ¶¨', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'çªç ´', 'å¼ºåŠ¿', 'æ–°é«˜', 'åšå¤š', 'æŠ„åº•', 'æ‹‰å‡', 'åå¼¹', 'æ¶¨åœ']
BEARISH_WORDS = ['è·Œ', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'ç ´ä½', 'å¼±åŠ¿', 'æ–°ä½', 'åšç©º', 'å‰²è‚‰', 'æ±ªæ±ª', 'å‰²äº†', 'æ‰“å‹', 'è·³æ°´']

def analyze_sentiment(text: str) -> Dict[str, Any]:
    """åˆ†ææƒ…ç»ª"""
    text_lower = text.lower()
    bull_count = sum(1 for w in BULLISH_WORDS if w in text_lower)
    bear_count = sum(1 for w in BEARISH_WORDS if w in text_lower)
    
    if bull_count > bear_count:
        return {"type": "åˆ©å¤š", "emoji": "ğŸŸ¢", "score": min(bull_count - bear_count, 5)}
    elif bear_count > bull_count:
        return {"type": "åˆ©ç©º", "emoji": "ğŸ”´", "score": min(bear_count - bull_count, 5)}
    return {"type": "ä¸­æ€§", "emoji": "âšª", "score": 0}

# ============ æ•°æ®æŠ“å– ============
def fetch_page(symbol: str, max_id: Optional[int] = None) -> List[Dict]:
    """æŠ“å–å•é¡µæ•°æ®"""
    url = f'https://xueqiu.com/query/v1/symbol/search/status?symbol={symbol}&count=20'
    if max_id:
        url += f'&max_id={max_id}'
    
    try:
        # æ‰“å¼€é¡µé¢
        r1 = subprocess.run(
            ['openclaw', 'browser', 'open', url],
            capture_output=True, text=True, timeout=30
        )
        
        match = re.search(r'id:\s*([A-F0-9]+)', r1.stdout)
        if not match:
            print(f"   âš ï¸ æ— æ³•è·å–é¡µé¢ID")
            return []
        
        target_id = match.group(1)
        time.sleep(2)  # ç­‰å¾…é¡µé¢åŠ è½½
        
        # è·å–å¿«ç…§
        r2 = subprocess.run(
            ['openclaw', 'browser', 'snapshot', '--target-id', target_id],
            capture_output=True, text=True, timeout=30
        )
        
        # å…³é—­é¡µé¢
        subprocess.run(
            ['openclaw', 'browser', 'close', '--target-id', target_id],
            capture_output=True, timeout=10
        )
        
        # è§£æJSONæ•°æ® - ä½¿ç”¨æ›´å¥å£®çš„æ–¹å¼
        # ä¿å­˜åŸå§‹è¾“å‡ºåˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åä½¿ç”¨æ­£åˆ™æå–
        import tempfile
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(r2.stdout)
            tmp_file = f.name
        
        try:
            with open(tmp_file, 'r') as f:
                content = f.read()
            
            os.unlink(tmp_file)
            
            # æ‰¾åˆ°JSONå¼€å§‹çš„ä½ç½®
            if 'generic [ref=' not in content:
                return []
            
            # æ ¼å¼: - generic [ref=e2]: "{...}"
            # æ‰¾åˆ° ": " åé¢çš„ "{ å¼€å¤´
            marker = 'generic [ref='
            pos = content.find(marker)
            if pos < 0:
                return []
            
            # ä»markerä½ç½®å‘åæ‰¾ ": "
            colon_pos = content.find('": "', pos)
            if colon_pos < 0:
                return []
            
            # JSONä» ": " åé¢å¼€å§‹ï¼Œä»¥ " ç»“å°¾
            start = colon_pos + 3  # è·³è¿‡ ": "
            
            # æ‰¾åˆ°è¡Œå°¾çš„ "
            end = content.find('"', start)
            if end < 0:
                end = len(content)
            
            if start >= end:
                return []
            
            json_str = content[start:end]
            
            try:
                data = json.loads(json_str)
                return data.get('list', [])
            except json.JSONDecodeError:
                # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                # æ›¿æ¢æœªè½¬ä¹‰çš„æ§åˆ¶å­—ç¬¦
                json_str = re.sub(r'[\x00-\x1F]', '', json_str)
                try:
                    data = json.loads(json_str)
                    return data.get('list', [])
                except:
                    return []
        except Exception as e:
            print(f"   è§£æé”™è¯¯: {e}")
            return []
        
    except Exception as e:
        print(f"   âŒ æŠ“å–é”™è¯¯: {e}")
        return []

def fetch_24h_posts(symbol: str, name: str) -> List[Dict]:
    """æŠ“å–24å°æ—¶å†…çš„æ‰€æœ‰è®¨è®º"""
    print(f"\nğŸ“¡ {name} ({symbol})")
    print("-" * 60)
    
    now_ts = datetime.now().timestamp() * 1000
    one_day_ms = 24 * 60 * 60 * 1000
    
    all_posts = []
    max_id = None
    page = 1
    
    while page <= MAX_PAGES:
        print(f"   æŠ“å–ç¬¬ {page} é¡µ...", end=" ")
        
        posts = fetch_page(symbol, max_id)
        if not posts:
            print("æ— æ•°æ®")
            break
        
        valid_count = 0
        stop_fetch = False
        
        for post in posts:
            ts = post.get('created_at', 0)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡24å°æ—¶
            if now_ts - ts > one_day_ms:
                stop_fetch = True
                break
            
            # æ¸…æ´—æ–‡æœ¬
            html_text = post.get('text', '')
            plain_text = re.sub(r'<[^>]+>', '', html_text)
            plain_text = plain_text.replace('&nbsp;', ' ').replace('&quot;', '"').strip()
            
            if len(plain_text) < 5:  # è¿‡æ»¤å¤ªçŸ­çš„
                continue
            
            # åˆ†ææƒ…ç»ª
            sentiment = analyze_sentiment(plain_text)
            
            all_posts.append({
                'id': post.get('id'),
                'text': plain_text,
                'author': post.get('user', {}).get('screen_name', 'åŒ¿å'),
                'timestamp': ts,
                'time_str': datetime.fromtimestamp(ts/1000).strftime('%m-%d %H:%M'),
                'likes': post.get('like_count', 0),
                'comments': post.get('reply_count', 0),
                'views': post.get('view_count', 0),
                'sentiment': sentiment,
            })
            valid_count += 1
        
        print(f"è·å– {valid_count} æ¡")
        
        if stop_fetch or valid_count < len(posts):
            print(f"   â° å·²è¶…å‡º24å°æ—¶æˆ–åˆ°è¾¾æœ«å°¾")
            break
        
        # ä¸‹ä¸€é¡µ
        max_id = posts[-1].get('id')
        page += 1
        time.sleep(SLEEP_TIME)
    
    print(f"   âœ… æ€»è®¡: {len(all_posts)} æ¡")
    return all_posts

# ============ ç”ŸæˆæŠ¥å‘Š ============
def generate_report(all_data: Dict[str, List[Dict]]) -> str:
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    now = datetime.now()
    report = f"""# ğŸ“Š é›ªçƒèˆ†æƒ…ç›‘æ§æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {now.strftime('%Y-%m-%d %H:%M:%S')}  
**ç›‘æ§è‚¡ç¥¨**: {len(SYMBOLS)} åª

---

"""
    
    for symbol, name in SYMBOLS:
        posts = all_data.get(symbol, [])
        
        # ç»Ÿè®¡
        bull = len([p for p in posts if p['sentiment']['type'] == 'åˆ©å¤š'])
        bear = len([p for p in posts if p['sentiment']['type'] == 'åˆ©ç©º'])
        neutral = len([p for p in posts if p['sentiment']['type'] == 'ä¸­æ€§'])
        
        report += f"""## ğŸ“ˆ {name} ({symbol})

**ç»Ÿè®¡**: å…± {len(posts)} æ¡ | ğŸŸ¢ {bull} | ğŸ”´ {bear} | âšª {neutral}

"""
        
        if posts:
            report += "### æœ€æ–°è®¨è®º\n\n"
            for i, p in enumerate(posts[:5], 1):
                report += f"{i}. {p['sentiment']['emoji']} **{p['time_str']}** | {p['author']}\n"
                report += f"   > {p['text'][:100]}{'...' if len(p['text']) > 100 else ''}\n\n"
        else:
            report += "*æš‚æ— æ•°æ®*\n\n"
        
        report += "---\n\n"
    
    return report

def save_results(all_data: Dict[str, List[Dict]], report: str):
    """ä¿å­˜ç»“æœ"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    # ä¿å­˜JSON
    json_file = os.path.join(OUTPUT_DIR, f'xueqiu_{timestamp}.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'fetch_time': datetime.now().isoformat(),
            'data': all_data
        }, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜MarkdownæŠ¥å‘Š
    md_file = os.path.join(OUTPUT_DIR, f'report_{timestamp}.md')
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
    print(f"   JSON: {json_file}")
    print(f"   æŠ¥å‘Š: {md_file}")

# ============ ä¸»ç¨‹åº ============
def main():
    print("=" * 60)
    print("ğŸ§ é›ªçƒèˆ†æƒ…ç›‘æ§ - å®Œæ•´ç‰ˆ v2.0")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print(f"\né…ç½®:")
    print(f"   è‚¡ç¥¨æ•°: {len(SYMBOLS)}")
    print(f"   æœ€å¤§é¡µæ•°: {MAX_PAGES}")
    print(f"   é™é€Ÿ: {SLEEP_TIME}ç§’/é¡µ")
    
    # æŠ“å–æ•°æ®
    all_data = {}
    for symbol, name in SYMBOLS:
        posts = fetch_24h_posts(symbol, name)
        all_data[symbol] = posts
        time.sleep(1)  # è‚¡ç¥¨é—´é—´éš”
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”ŸæˆæŠ¥å‘Š...")
    print("=" * 60)
    
    report = generate_report(all_data)
    save_results(all_data, report)
    
    # æ‰“å°æ‘˜è¦
    print("\nğŸ“ˆ ç›‘æ§æ‘˜è¦:")
    print("-" * 60)
    for symbol, name in SYMBOLS:
        posts = all_data.get(symbol, [])
        bull = len([p for p in posts if p['sentiment']['type'] == 'åˆ©å¤š'])
        bear = len([p for p in posts if p['sentiment']['type'] == 'åˆ©ç©º'])
        print(f"   {name}: {len(posts)}æ¡ (ğŸŸ¢{bull} ğŸ”´{bear})")
    
    print("\n" + "=" * 60)
    print("âœ… ç›‘æ§å®Œæˆ!")
    print("=" * 60)

if __name__ == "__main__":
    main()
