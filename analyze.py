#!/usr/bin/env python3
"""
èˆ†æƒ…åˆ†ææ¨¡å— - LLMé©±åŠ¨
ä½¿ç”¨LLMå¯¹é›ªçƒå†…å®¹è¿›è¡Œæ·±åº¦åˆ†æ
"""

import json
import sys
import os
import time
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import LLM_MODEL, TEMPERATURE

# é…ç½®ï¼ˆä»config.pyè¯»å–ï¼‰
LLM_MODEL_CONFIG = LLM_MODEL  # "minimax/MiniMax-M2.1" æˆ– "moonshot/kimi-k2.5"
MAX_TOKENS = 1000

def get_llm_client():
    """
    è·å–LLMå®¢æˆ·ç«¯
    ä¼˜å…ˆä½¿ç”¨ MiniMaxï¼Œå…¼å®¹ OpenAI
    """
    import json
    import os
    
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
    api_key = os.environ.get("MINIMAX_API_KEY")
    
    # å¦‚æœæ²¡æœ‰ç¯å¢ƒå˜é‡ï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–
    if not api_key:
        # æ£€æŸ¥ç”¨æˆ·ç›®å½•çš„é…ç½®æ–‡ä»¶
        key_file = os.path.expanduser("~/.config/minimax_api_key")
        if os.path.exists(key_file):
            with open(key_file, 'r') as f:
                api_key = f.read().strip()
    
    if api_key:
        try:
            from openai import OpenAI
            client = OpenAI(
                api_key=api_key,
                base_url="https://api.minimax.chat/v1/text/chatcompletion_v2"
            )
            return client, "minimax"
        except Exception as e:
            print(f"âš ï¸ MiniMaxå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    # å°è¯• OpenAI
    try:
        from openai import OpenAI
        client = OpenAI()
        return client, "openai"
    except Exception as e:
        print(f"âš ï¸ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return None, None

# èˆ†æƒ…åˆ†æPromptï¼ˆæ ¸å¿ƒï¼‰
ANALYZE_PROMPT = """ä½ æ˜¯ä¸€åAè‚¡äºŒçº§å¸‚åœºèˆ†æƒ…åˆ†æå‘˜ï¼ŒæœåŠ¡å¯¹è±¡æ˜¯çŸ­çº¿å’Œæ³¢æ®µäº¤æ˜“ã€‚

è¯·åŸºäºä»¥ä¸‹é›ªçƒç”¨æˆ·å†…å®¹è¿›è¡Œåˆ†æï¼Œå¹¶è¾“å‡ºç»“æ„åŒ–JSONç»“æœã€‚

ã€åˆ†æè¦æ±‚ã€‘
1. æƒ…ç»ªæ–¹å‘ï¼šå¤š / ç©º / ä¸­æ€§
2. æƒ…ç»ªå¼ºåº¦ï¼š1-5ï¼ˆ5ä¸ºæå¼ºï¼‰
3. é¢„æœŸå˜åŒ–ï¼šé¢„æœŸä¸Šä¿® / é¢„æœŸä¸‹ä¿® / åˆ†æ­§åŠ å¤§ / æ— æ˜æ˜¾å˜åŒ–
4. ä¿¡æ¯ç±»å‹ï¼šä¸šç»© / æ”¿ç­– / èµ„é‡‘ / äº‹ä»¶/ä¼ é—» / æƒ…ç»ªå®£æ³„ / å…¶ä»–
5. æ˜¯å¦å±äºé‡å¤ä¿¡æ¯æˆ–å™ªéŸ³ï¼ˆæ˜¯/å¦ï¼‰
6. åˆ¤æ–­è¯¥ä¿¡æ¯æ˜¯å¦å¯èƒ½é¢†å…ˆä»·æ ¼ï¼ˆæ˜¯/å¦ï¼‰
7. ç”¨ä¸€å¥è¯æ€»ç»“å¯¹æœªæ¥3-5ä¸ªäº¤æ˜“æ—¥è‚¡ä»·çš„æ½œåœ¨å½±å“

ã€æ³¨æ„ã€‘
- ä¸è¦å¤è¿°åŸæ–‡
- èšç„¦"æ˜¯å¦å½±å“äº¤æ˜“å†³ç­–"
- å¦‚æœæ˜¯æƒ…ç»ªå™ªéŸ³ï¼Œè¯·æ˜ç¡®æŒ‡å‡º
- è¾“å‡ºå¿…é¡»æ˜¯çº¯JSONï¼Œä¸è¦åŒ…å«markdownä»£ç å—
- å¦‚æœå†…å®¹å¤ªçŸ­æ— æ³•åˆ†æï¼Œè¿”å› {"error": "å†…å®¹è¿‡çŸ­"}

ã€å†…å®¹ã€‘
{text}
"""

def analyze_with_llm(text: str, client=None, provider=None) -> Dict:
    """
    ä½¿ç”¨LLMåˆ†æå•æ¡å†…å®¹
    
    Args:
        text: è¦åˆ†æçš„æ–‡æœ¬
        client: LLMå®¢æˆ·ç«¯
        provider: ä¾›åº”å•† (minimax/openai)
    
    Returns:
        dict: åˆ†æç»“æœ
    """
    if not text or len(text.strip()) < 10:
        return {"error": "å†…å®¹è¿‡çŸ­"}
    
    # ç®€å•å…³é”®è¯åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    keyword_analysis = simple_keyword_analysis(text)
    if keyword_analysis:
        return keyword_analysis
    
    # æ„å»ºæ¶ˆæ¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Aè‚¡èˆ†æƒ…åˆ†æå¸ˆï¼Œè¾“å‡ºå¿…é¡»æ˜¯ä¸¥æ ¼çš„JSONæ ¼å¼ã€‚"},
        {"role": "user", "content": ANALYZE_PROMPT.format(text=text[:2000])}  # é™åˆ¶é•¿åº¦
    ]
    
    try:
        # è·å–å®¢æˆ·ç«¯
        if client is None:
            client, provider = get_llm_client()
        
        if client is None:
            return {"error": "æ— æ³•åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"}
        
        # ç¡®å®šæ¨¡å‹åç§°
        if provider == "minimax":
            model_name = LLM_MODEL_CONFIG.replace("minimax/", "")
        else:
            model_name = LLM_MODEL_CONFIG
        
        resp = client.chat.completions.create(
            model=model_name,
            messages=messages,
            temperature=TEMPERATURE,
            max_tokens=MAX_TOKENS,
        )
        
        content = resp.choices[0].message.content
        
        # æ¸…ç†å¹¶è§£æJSON
        content = content.strip()
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        content = content.replace("```json", "").replace("```", "").strip()
        
        result = json.loads(content)
        
        # éªŒè¯å¿…è¦å­—æ®µ
        if "sentiment" not in result:
            return {"error": "è§£æç»“æœç¼ºå°‘å¿…è¦å­—æ®µ"}
        
        return result
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
        return {"error": f"JSONè§£æå¤±è´¥: {str(e)}"}
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return {"error": str(e)}


def simple_keyword_analysis(text: str) -> Dict:
    """
    ç®€å•å…³é”®è¯åˆ†æï¼ˆæ— LLMæ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰
    åŸºäºå…³é”®è¯åˆ¤æ–­æƒ…ç»ª
    """
    text_lower = text.lower()
    
    # å…³é”®è¯
    positive_words = ['æ¶¨', 'çœ‹å¥½', 'ä¹°å…¥', 'åŠ ä»“', 'åˆ©å¥½', 'çªç ´', 'æ–°é«˜', 'åšå¤š', 'æŠ„åº•', 'ä½å¸', 'é‡‘å‰', 'æ”¾é‡']
    negative_words = ['è·Œ', 'çœ‹ç©º', 'å–å‡º', 'å‡ä»“', 'åˆ©ç©º', 'ç ´ä½', 'æ–°ä½', 'åšç©º', 'å‰²è‚‰', 'é«˜æŠ›', 'æ­»å‰', 'ç¼©é‡', 'è¢«å¥—', 'äºæŸ']
    
    pos_count = sum(1 for w in positive_words if w in text_lower)
    neg_count = sum(1 for w in negative_words if w in text_lower)
    
    if pos_count > neg_count:
        sentiment = "å¤š"
        intensity = min(1 + pos_count, 5)
    elif neg_count > pos_count:
        sentiment = "ç©º"
        intensity = min(1 + neg_count, 5)
    else:
        sentiment = "ä¸­æ€§"
        intensity = 1
    
    return {
        "sentiment": sentiment,
        "intensity": intensity,
        "expectation": "æ— æ˜æ˜¾å˜åŒ–",
        "info_type": "å…¶ä»–",
        "noise": "å¦",
        "leading": "å¦",
        "summary": "åŸºäºå…³é”®è¯çš„ç®€å•åˆ†æ",
        "_method": "keyword"
    }

def batch_analyze(items: List[Dict], limit: int = 50) -> List[Dict]:
    """
    æ‰¹é‡åˆ†æèˆ†æƒ…å†…å®¹
    
    Args:
        items: æ ‡å‡†åŒ–åçš„æ•°æ®åˆ—è¡¨
        limit: æœ€å¤§åˆ†ææ•°é‡
    
    Returns:
        list: å¸¦åˆ†æç»“æœçš„æ•°æ®åˆ—è¡¨
    """
    client, provider = get_llm_client()
    
    use_keyword = client is None
    if use_keyword:
        print(f"\nğŸ” å¼€å§‹åˆ†æ {min(len(items), limit)} æ¡å†…å®¹ (ä½¿ç”¨å…³é”®è¯åˆ†æ)...")
    else:
        print(f"\nğŸ” å¼€å§‹åˆ†æ {min(len(items), limit)} æ¡å†…å®¹ (ä½¿ç”¨ {provider})...")
    
    results = []
    count = 0
    
    for item in items[:limit]:
        if count >= limit:
            break
            
        text = item.get("text", "")
        if not text or len(text.strip()) < 10:
            continue
        
        print(f"  åˆ†æ [{count+1}/{min(len(items), limit)}]: {text[:30]}...")
        
        # åˆ†æ
        if use_keyword:
            analysis = simple_keyword_analysis(text)
        else:
            analysis = analyze_with_llm(text, client, provider)
        
        item["analysis"] = analysis
        
        results.append(item)
        count += 1
        
        # æ§é€Ÿ
        sleep_time = 1.5 if provider == "minimax" else 1.2
        time.sleep(0.3 if use_keyword else sleep_time)
    
    print(f"\nâœ… åˆ†æå®Œæˆ: {len(results)} æ¡")
    return results

def calculate_weight(item: Dict) -> float:
    """
    è®¡ç®—èˆ†æƒ…æƒé‡åˆ†
    
    å…¬å¼: æƒ…ç»ªå¼ºåº¦ Ã— é¢„æœŸå˜åŒ–ç³»æ•° Ã— æ˜¯å¦é¢†å…ˆ Ã— æ¥æºæƒé‡ Ã— å¸‚åœºç¯å¢ƒ
    
    Args:
        item: å¸¦åˆ†æç»“æœçš„æ•°æ®é¡¹
    
    Returns:
        float: æƒé‡åˆ†
    """
    analysis = item.get("analysis", {})
    
    if "error" in analysis:
        return 0.0
    
    # 1. æƒ…ç»ªå¼ºåº¦ (1-5)
    intensity = analysis.get("intensity", 1)
    
    # 2. é¢„æœŸå˜åŒ–ç³»æ•°
    expectation_map = {
        "é¢„æœŸä¸Šä¿®": 1.0,
        "é¢„æœŸä¸‹ä¿®": 1.0,  # ç©ºå¤´ä¿¡æ¯åŒæ ·æœ‰ä»·å€¼
        "åˆ†æ­§åŠ å¤§": 0.7,
        "æ— æ˜æ˜¾å˜åŒ–": 0.5,  # æ”¹ä¸º0.5ï¼Œé¿å…å…³é”®è¯åˆ†ææ•°æ®è¢«è¿‡æ»¤
    }
    expectation_coef = expectation_map.get(analysis.get("expectation", "æ— æ˜æ˜¾å˜åŒ–"), 0.5)
    
    # 3. æ˜¯å¦é¢†å…ˆä»·æ ¼
    leading = 1.5 if analysis.get("leading") == "æ˜¯" else 0.7
    
    # 4. æ¥æºæƒé‡
    source_weights = {
        "status": 1.0,  # æ™®é€šå¸–å­
        "livenews": 1.2,  # å¿«è®¯æƒé‡æ›´é«˜
    }
    source_weight = source_weights.get(item.get("type"), 1.0)
    
    # 5. å™ªéŸ³è¿‡æ»¤
    if analysis.get("noise") == "æ˜¯":
        return 0.0
    
    # è®¡ç®—æƒé‡
    weight = intensity * expectation_coef * leading * source_weight
    
    return round(weight, 2)

def enrich_with_weights(analyzed_items: List[Dict]) -> List[Dict]:
    """
    ä¸ºåˆ†æç»“æœæ·»åŠ æƒé‡
    
    Args:
        analyzed_items: å·²åˆ†æçš„æ•°æ®åˆ—è¡¨
    
    Returns:
        list: æ·»åŠ æƒé‡åçš„æ•°æ®
    """
    for item in analyzed_items:
        item["weight"] = calculate_weight(item)
    
    # è¿‡æ»¤é›¶æƒé‡ï¼ˆå™ªéŸ³ï¼‰
    filtered = [i for i in analyzed_items if i.get("weight", 0) > 0]
    
    return filtered

def save_analyzed_data(data: List[Dict], filename: str = "/tmp/xueqiu_analyzed.jsonl"):
    """
    ä¿å­˜åˆ†æç»“æœ
    
    Args:
        data: åˆ†æåçš„æ•°æ®
        filename: è¾“å‡ºæ–‡ä»¶
    """
    with open(filename, "w", encoding="utf-8") as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"ğŸ’¾ å·²ä¿å­˜ {len(data)} æ¡åˆ†æç»“æœåˆ° {filename}")

if __name__ == "__main__":
    from normalize import load_raw_data
    from config import SYMBOLS
    
    print("=" * 60)
    print("ğŸ§  èˆ†æƒ…åˆ†æï¼ˆLLMé©±åŠ¨ï¼‰")
    print("=" * 60)
    
    # åŠ è½½æ ‡å‡†åŒ–æ•°æ®
    raw_file = "/tmp/xueqiu_normalized.jsonl"
    
    if not os.path.exists(raw_file):
        print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°æ ‡å‡†åŒ–æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ normalize.py")
        sys.exit(1)
    
    # è¯»å–æ•°æ®
    items = []
    with open(raw_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    
    print(f"ğŸ“¥ åŠ è½½ {len(items)} æ¡æ ‡å‡†åŒ–æ•°æ®")
    
    # æ‰¹é‡åˆ†æï¼ˆé™åˆ¶æ•°é‡ä»¥æ§åˆ¶æˆæœ¬ï¼‰
    analyzed = batch_analyze(items, limit=30)
    
    # æ·»åŠ æƒé‡
    enriched = enrich_with_weights(analyzed)
    
    # ä¿å­˜
    save_analyzed_data(enriched)
    
    # ç»Ÿè®¡
    positive = len([i for i in enriched if i.get("analysis", {}).get("sentiment") == "å¤š"])
    negative = len([i for i in enriched if i.get("analysis", {}).get("sentiment") == "ç©º"])
    neutral = len([i for i in enriched if i.get("analysis", {}).get("sentiment") == "ä¸­æ€§"])
    
    print(f"\nğŸ“Š æƒ…ç»ªç»Ÿè®¡:")
    print(f"  - å¤š: {positive} æ¡")
    print(f"  - ç©º: {negative} æ¡")
    print(f"  - ä¸­: {neutral} æ¡")
