#!/usr/bin/env python3
"""
é›ªçƒèˆ†æƒ…ç›‘æ§ - ä¸»å…¥å£
==========================

åŠŸèƒ½:
1. æŠ“å–é›ªçƒä¸ªè‚¡è®¨è®ºå’Œå¿«è®¯
2. LLMèˆ†æƒ…åˆ†æ
3. ç”Ÿæˆäº¤æ˜“ä¿¡å·
4. Top10èšåˆ
5. æ¨é€é€šçŸ¥

ä½¿ç”¨:
    python run.py              # å®Œæ•´æµç¨‹
    python run.py --fetch      # ä»…æŠ“å–
    python run.py --analyze    # ä»…åˆ†æ
    python run.py --signals    # ä»…ç”Ÿæˆä¿¡å·
    python run.py --top10      # ä»…èšåˆTop10
    python run.py --send       # ä»…æ¨é€
"""

import sys
import os
import json
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import SYMBOLS

def step_fetch():
    """Step 1: æŠ“å–æ•°æ®"""
    print("\n" + "=" * 60)
    print("ğŸ“¥ Step 1: æŠ“å–é›ªçƒæ•°æ®")
    print("=" * 60)
    
    from fetch_status import fetch_discussions
    from fetch_livenews import fetch_livenews
    from normalize import save_normalized_data
    
    # æŠ“å–ä¸ªè‚¡è®¨è®º
    print(f"\nğŸ£ æŠ“å– {len(SYMBOLS)} åªè‚¡ç¥¨çš„è®¨è®º...")
    status_data = []
    for symbol in SYMBOLS:
        posts = fetch_discussions(symbol)
        status_data.extend(posts)
    
    print(f"   è·å– {len(status_data)} æ¡è®¨è®º")
    
    # æŠ“å–å¿«è®¯
    print("\nğŸ“° æŠ“å–é›ªçƒå¿«è®¯...")
    livenews_data = fetch_livenews(50)
    print(f"   è·å– {len(livenews_data)} æ¡å¿«è®¯")
    
    # æ ‡å‡†åŒ–
    from normalize import normalize_all
    print("\nğŸ”§ æ ‡å‡†åŒ–æ•°æ®...")
    normalized = normalize_all(status_data, livenews_data, SYMBOLS)
    print(f"   æ ‡å‡†åŒ– {len(normalized)} æ¡")
    
    # ä¿å­˜
    save_normalized_data(normalized)
    
    return len(normalized)

def step_analyze():
    """Step 2: LLMåˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ§  Step 2: LLMèˆ†æƒ…åˆ†æ")
    print("=" * 60)
    
    normalized_file = "/tmp/xueqiu_normalized.jsonl"
    
    if not os.path.exists(normalized_file):
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ ‡å‡†åŒ–æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ --fetch")
        return 0
    
    # è¯»å–æ•°æ®
    items = []
    with open(normalized_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    
    print(f"ğŸ“¥ åŠ è½½ {len(items)} æ¡æ•°æ®")
    
    # åˆ†æ
    from analyze import batch_analyze, enrich_with_weights, save_analyzed_data
    
    analyzed = batch_analyze(items, limit=50)
    enriched = enrich_with_weights(analyzed)
    save_analyzed_data(enriched)
    
    # ç»Ÿè®¡
    positive = len([i for i in enriched if i.get("analysis", {}).get("sentiment") == "å¤š"])
    negative = len([i for i in enriched if i.get("analysis", {}).get("sentiment") == "ç©º"])
    neutral = len([i for i in enriched if i.get("analysis", {}).get("sentiment") == "ä¸­æ€§"])
    
    print(f"\nğŸ“Š æƒ…ç»ªç»Ÿè®¡:")
    print(f"   ğŸŸ¢ å¤š: {positive} æ¡")
    print(f"   ğŸ”´ ç©º: {negative} æ¡")
    print(f"   âšª ä¸­: {neutral} æ¡")
    
    return len(enriched)

def step_signals():
    """Step 3: ç”Ÿæˆä¿¡å·"""
    print("\n" + "=" * 60)
    print("ğŸš¨ Step 3: ç”Ÿæˆäº¤æ˜“ä¿¡å·")
    print("=" * 60)
    
    analyzed_file = "/tmp/xueqiu_analyzed.jsonl"
    
    if not os.path.exists(analyzed_file):
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åˆ†ææ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ --fetch --analyze")
        return 0
    
    # è¯»å–
    items = []
    with open(analyzed_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    
    print(f"ğŸ“¥ åŠ è½½ {len(items)} æ¡åˆ†ææ•°æ®")
    
    # è·å–ä»·æ ¼
    from signals import SentimentSignals, get_price_changes
    price_changes = get_price_changes(SYMBOLS)
    print(f"ğŸ“ˆ è·å– {len(price_changes)} åªè‚¡ç¥¨ä»·æ ¼")
    
    # æ£€æµ‹ä¿¡å·
    detector = SentimentSignals()
    signals = detector.detect_all(items, price_changes)
    
    print(f"\nğŸš¨ æ£€æµ‹åˆ° {len(signals)} ä¸ªä¿¡å·:")
    for i, signal in enumerate(signals[:10], 1):
        emoji = {"æœºä¼šå‹": "ğŸŸ¢", "é£é™©å‹": "ğŸ”´", "éªŒè¯å‹": "ğŸŸ¡"}.get(signal.get("type"), "âšª")
        print(f"   {i}. {emoji} {signal['symbol']} | {signal['signal']}")
        print(f"      {signal['reason']}")
    
    # ä¿å­˜
    with open("/tmp/xueqiu_signals.json", "w", encoding="utf-8") as f:
        json.dump(signals, f, ensure_ascii=False, indent=2)
    
    return len(signals)

def step_top10():
    """Step 4: ç”ŸæˆTop10"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Step 4: ç”ŸæˆTop10èˆ†æƒ…")
    print("=" * 60)
    
    analyzed_file = "/tmp/xueqiu_analyzed.jsonl"
    
    if not os.path.exists(analyzed_file):
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°åˆ†ææ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ --fetch --analyze")
        return 0
    
    # è¯»å–
    items = []
    with open(analyzed_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    
    print(f"ğŸ“¥ åŠ è½½ {len(items)} æ¡åˆ†ææ•°æ®")
    
    # èšåˆ
    from top10 import aggregate_by_symbol, generate_top10
    from signals import get_price_changes
    
    aggregated = aggregate_by_symbol(items)
    print(f"ğŸ“Š èšåˆä¸º {len(aggregated)} åªè‚¡ç¥¨")
    
    # è·å–ä»·æ ¼
    price_changes = get_price_changes(SYMBOLS)
    
    # ç”ŸæˆTop10
    top10 = generate_top10(aggregated, price_changes, limit=10)
    
    print(f"\nğŸ¯ Top10èˆ†æƒ…è‚¡ç¥¨:")
    for item in top10:
        emoji = {"æœºä¼šå‹": "ğŸŸ¢", "é£é™©å‹": "ğŸ”´", "éªŒè¯å‹": "ğŸŸ¡", "å…³æ³¨å‹": "ğŸŸ "}.get(item.get("type"), "âšª")
        print(f"   {item['rank']}. {emoji} {item['symbol']} | {item['type']}")
        print(f"      {item['reason']}")
    
    # ä¿å­˜
    with open("/tmp/xueqiu_top10.json", "w", encoding="utf-8") as f:
        json.dump(top10, f, ensure_ascii=False, indent=2)
    
    return len(top10)

def step_send():
    """Step 5: æ¨é€"""
    print("\n" + "=" * 60)
    print("ğŸ“¤ Step 5: æ¨é€åˆ°Telegram")
    print("=" * 60)
    
    from send_telegram import send_top10, send_signals
    
    success = 0
    
    if os.path.exists("/tmp/xueqiu_top10.json"):
        if send_top10():
            success += 1
    
    if os.path.exists("/tmp/xueqiu_signals.json"):
        if send_signals():
            success += 1
    
    return success

def main():
    parser = argparse.ArgumentParser(description="é›ªçƒèˆ†æƒ…ç›‘æ§")
    parser.add_argument("--fetch", action="store_true", help="ä»…æŠ“å–æ•°æ®")
    parser.add_argument("--analyze", action="store_true", help="ä»…åˆ†æ")
    parser.add_argument("--signals", action="store_true", help="ä»…ç”Ÿæˆä¿¡å·")
    parser.add_argument("--top10", action="store_true", help="ä»…ç”ŸæˆTop10")
    parser.add_argument("--send", action="store_true", help="ä»…æ¨é€")
    parser.add_argument("--all", action="store_true", help="å®Œæ•´æµç¨‹")
    
    args = parser.parse_args()
    
    # é»˜è®¤å®Œæ•´æµç¨‹
    if not any([args.fetch, args.analyze, args.signals, args.top10, args.send]):
        args.all = True
    
    print("\n" + "=" * 60)
    print("ğŸ§ é›ªçƒèˆ†æƒ…ç›‘æ§ç³»ç»Ÿ")
    print(f"â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print(f"ğŸ“¦ ç›‘æ§ {len(SYMBOLS)} åªè‚¡ç¥¨: {', '.join(SYMBOLS)}")
    
    # æ‰§è¡Œæ­¥éª¤
    stats = {}
    
    if args.fetch or args.all:
        stats["fetched"] = step_fetch()
    
    if args.analyze or args.all:
        stats["analyzed"] = step_analyze()
    
    if args.signals or args.all:
        stats["signals"] = step_signals()
    
    if args.top10 or args.all:
        stats["top10"] = step_top10()
    
    if args.send or args.all:
        stats["sent"] = step_send()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆç»Ÿè®¡")
    print("=" * 60)
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    print("\nğŸ’¡ æ–‡ä»¶ä½ç½®:")
    print("   - æ ‡å‡†åŒ–æ•°æ®: /tmp/xueqiu_normalized.jsonl")
    print("   - åˆ†æç»“æœ: /tmp/xueqiu_analyzed.jsonl")
    print("   - ä¿¡å·: /tmp/xueqiu_signals.json")
    print("   - Top10: /tmp/xueqiu_top10.json")

if __name__ == "__main__":
    main()
