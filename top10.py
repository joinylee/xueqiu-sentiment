#!/usr/bin/env python3
"""
Top10èˆ†æƒ…èšåˆæ¨¡å—
æ¯å¤©åªå…³æ³¨"ä¿¡æ¯å¯†åº¦å¼‚å¸¸"çš„è‚¡ç¥¨
"""

import json
import sys
import os
from datetime import datetime
from typing import Dict, List
from collections import defaultdict

def calculate_top_score(stock_data: Dict) -> float:
    """
    è®¡ç®—Top10ç»¼åˆå¾—åˆ†
    
    å…¬å¼: 0.4Ã—Total + 0.3Ã—Acceleration + 0.2Ã—|BiasShift| + 0.1Ã—Danger
    """
    # è·å–æŒ‡æ ‡
    total_score = stock_data.get("total_score", 0)
    acceleration = stock_data.get("acceleration", 0)
    bias_shift = stock_data.get("bias_shift", 0)
    danger = stock_data.get("danger", 0)
    
    # ç®€å•å½’ä¸€åŒ–ï¼ˆä½¿ç”¨ç™¾åˆ†ä½ä¼šæ›´å‡†ç¡®ï¼‰
    def normalize(val, max_val=100):
        return min(val / max_val, 1.0) if max_val > 0 else 0
    
    # è®¡ç®—åŠ æƒå¾—åˆ†
    score = (
        0.4 * normalize(total_score, 50) +
        0.3 * normalize(acceleration, 5) +
        0.2 * abs(normalize(bias_shift, 1)) +
        0.1 * normalize(danger, 10)
    )
    
    return round(score, 3)

def aggregate_by_symbol(analyzed_data: List[Dict], time_window_hours: int = 2) -> List[Dict]:
    """
    æŒ‰è‚¡ç¥¨èšåˆèˆ†æƒ…æ•°æ®
    
    Args:
        analyzed_data: åˆ†æåçš„èˆ†æƒ…æ•°æ®
        time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
    
    Returns:
        list: èšåˆåçš„è‚¡ç¥¨æ•°æ®
    """
    now = datetime.now().timestamp()
    cutoff = now - time_window_hours * 3600
    
    # æŒ‰è‚¡ç¥¨åˆ†ç»„
    by_symbol = defaultdict(lambda: {
        "items": [],
        "recent_items": [],
        "total_score": 0,
        "positive_count": 0,
        "negative_count": 0,
        "leading_count": 0,
        "total_weight": 0,
    })
    
    for item in analyzed_data:
        symbol = item.get("symbol")
        if not symbol:
            continue
        
        timestamp = item.get("timestamp", 0)
        weight = item.get("weight", 0)
        analysis = item.get("analysis", {})
        
        data = by_symbol[symbol]
        data["symbol"] = symbol
        data["items"].append(item)
        data["total_score"] += weight
        
        if timestamp > cutoff:
            data["recent_items"].append(item)
        
        if analysis.get("sentiment") == "å¤š":
            data["positive_count"] += 1
        elif analysis.get("sentiment") == "ç©º":
            data["negative_count"] += 1
        
        if analysis.get("leading") == "æ˜¯":
            data["leading_count"] += 1
        
        data["total_weight"] += weight
    
    # è®¡ç®—èšåˆæŒ‡æ ‡
    result = []
    
    for symbol, data in by_symbol.items():
        items = data["items"]
        recent = data["recent_items"]
        
        if not items:
            continue
        
        # èˆ†æƒ…åŠ é€Ÿåº¦ = æœ€è¿‘30åˆ†é’Ÿæƒé‡ Ã· è¿‡å»2å°æ—¶å¹³å‡
        recent_30min = [i for i in items if i.get("timestamp", 0) > now - 1800]
        if len(items) > 5 and sum(i.get("weight", 0) for i in items) > 0:
            avg_weight = data["total_weight"] / len(items)
            recent_weight = sum(i.get("weight", 0) for i in recent_30min) / max(len(recent_30min), 1)
            acceleration = recent_weight / avg_weight if avg_weight > 0 else 0
        else:
            acceleration = 1.0
        
        # æƒ…ç»ªåç§» = è¿‘æœŸæƒ…ç»ª - æ•´ä½“æƒ…ç»ª
        recent_positive = len([i for i in recent if i.get("analysis", {}).get("sentiment") == "å¤š"])
        recent_negative = len([i for i in recent if i.get("analysis", {}).get("sentiment") == "ç©º"])
        
        if len(recent) > 0:
            recent_bias = (recent_positive - recent_negative) / len(recent)
        else:
            recent_bias = 0
        
        overall_positive = data["positive_count"]
        overall_negative = data["negative_count"]
        
        if len(items) > 0:
            overall_bias = (overall_positive - overall_negative) / len(items)
        else:
            overall_bias = 0
        
        bias_shift = recent_bias - overall_bias
        
        # åˆ†æ­§åº¦ = å¤šå¤´å¼ºåº¦ Ã— ç©ºå¤´å¼ºåº¦
        positive_intensity = sum(
            i.get("analysis", {}).get("intensity", 0) 
            for i in items if i.get("analysis", {}).get("sentiment") == "å¤š"
        )
        negative_intensity = sum(
            abs(i.get("analysis", {}).get("intensity", 0)) 
            for i in items if i.get("analysis", {}).get("sentiment") == "ç©º"
        )
        danger = (positive_intensity / max(data["positive_count"], 1)) * \
                  (negative_intensity / max(data["negative_count"], 1)) if data["positive_count"] and data["negative_count"] else 0
        
        stock_data = {
            "symbol": symbol,
            "total_score": round(data["total_score"], 2),
            "item_count": len(items),
            "acceleration": round(acceleration, 2),
            "bias_shift": round(bias_shift, 3),
            "danger": round(danger, 2),
            "positive_count": data["positive_count"],
            "negative_count": data["negative_count"],
            "leading_count": data["leading_count"],
            "items": data["items"][:10],  # åªä¿ç•™å‰10æ¡
        }
        
        result.append(stock_data)
    
    # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
    for stock in result:
        stock["top_score"] = calculate_top_score(stock)
    
    result.sort(key=lambda x: x["top_score"], reverse=True)
    
    return result

def assign_type(stock_data: Dict, price_change: float = 0) -> str:
    """
    ä¸ºè‚¡ç¥¨åˆ†é…ç”¨é€”ç±»å‹
    """
    score = stock_data["top_score"]
    acceleration = stock_data["acceleration"]
    bias_shift = stock_data["bias_shift"]
    danger = stock_data["danger"]
    item_count = stock_data["item_count"]
    
    # æœºä¼šå‹: èˆ†æƒ…å‡æ¸© + åå¤š + ä»·æ ¼æœªåŠ¨
    if acceleration > 1.5 and bias_shift > 0.1 and abs(price_change) < 1.0 and score > 0.3:
        return "æœºä¼šå‹"
    
    # é£é™©å‹: æƒ…ç»ªæç«¯æˆ–åˆ†æ­§æ”¾å¤§
    if danger > 1.5 or (stock_data["positive_count"] / item_count > 0.8 and score > 0.4):
        return "é£é™©å‹"
    
    # éªŒè¯å‹: èˆ†æƒ…ä¸ä»·æ ¼åŒæ­¥
    if abs(price_change) > 2 and bias_shift * price_change > 0:
        return "éªŒè¯å‹"
    
    # å…³æ³¨å‹: ç»¼åˆå¾—åˆ†è¿˜å¯ä»¥
    if score > 0.2:
        return "å…³æ³¨å‹"
    
    return "æ™®é€š"

def generate_top10(aggregated_data: List[Dict], price_changes: Dict[str, float], limit: int = 10) -> List[Dict]:
    """
    ç”ŸæˆTop10èˆ†æƒ…åˆ—è¡¨
    
    Args:
        aggregated_data: èšåˆåçš„æ•°æ®
        price_changes: è‚¡ç¥¨æ¶¨è·Œå¹…
        limit: è¿”å›æ•°é‡
    
    Returns:
        list: Top10åˆ—è¡¨
    """
    top10 = []
    
    for stock in aggregated_data[:limit * 2]:  # å…ˆå–æ›´å¤š
        symbol = stock["symbol"]
        price_change = price_changes.get(symbol, 0)
        
        stock_type = assign_type(stock, price_change)
        
        # ç”ŸæˆåŸå› 
        reasons = []
        
        if stock["acceleration"] > 1.5:
            reasons.append(f"èˆ†æƒ…åŠ é€Ÿåº¦â†‘({stock['acceleration']}x)")
        
        if stock["bias_shift"] > 0.2:
            reasons.append(f"æƒ…ç»ªåç§»â†‘({stock['bias_shift']:.2f})")
        
        if stock["leading_count"] > 0:
            reasons.append(f"é¢†å…ˆä¿¡å·{stock['leading_count']}æ¡")
        
        if stock["item_count"] > 10:
            reasons.append(f"è®¨è®º{item_count}æ¡")
        
        reason = "ï¼Œ".join(reasons[:2]) if reasons else "ç»¼åˆèˆ†æƒ…å…³æ³¨"
        
        top10.append({
            "rank": len(top10) + 1,
            "symbol": symbol,
            "type": stock_type,
            "reason": reason,
            "top_score": stock["top_score"],
            "price_change": price_change,
            "total_score": stock["total_score"],
            "item_count": stock["item_count"],
        })
        
        if len(top10) >= limit:
            break
    
    return top10

if __name__ == "__main__":
    from normalize import load_raw_data
    from analyze import batch_analyze, enrich_with_weights
    from signals import get_price_changes
    
    print("=" * 60)
    print("ğŸ“Š Top10èˆ†æƒ…èšåˆ")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    analyzed_file = "/tmp/xueqiu_analyzed.jsonl"
    
    if not os.path.exists(analyzed_file):
        print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°åˆ†ææ•°æ®")
        sys.exit(1)
    
    # è¯»å–
    items = []
    with open(analyzed_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                items.append(json.loads(line))
    
    print(f"ğŸ“¥ åŠ è½½ {len(items)} æ¡åˆ†ææ•°æ®")
    
    # èšåˆ
    aggregated = aggregate_by_symbol(items)
    print(f"ğŸ“Š èšåˆä¸º {len(aggregated)} åªè‚¡ç¥¨")
    
    # è·å–ä»·æ ¼
    from config import SYMBOLS
    price_changes = get_price_changes(SYMBOLS)
    print(f"ğŸ“ˆ è·å– {len(price_changes)} åªè‚¡ç¥¨ä»·æ ¼")
    
    # ç”ŸæˆTop10
    top10 = generate_top10(aggregated, price_changes)
    
    print(f"\nğŸ¯ Top10èˆ†æƒ…è‚¡ç¥¨:")
    for item in top10:
        emoji = {"æœºä¼šå‹": "ğŸŸ¢", "é£é™©å‹": "ğŸ”´", "éªŒè¯å‹": "ğŸŸ¡", "å…³æ³¨å‹": "ğŸŸ¡"}.get(item["type"], "âšª")
        print(f"  {item['rank']}. {emoji} {item['symbol']} | {item['type']}")
        print(f"     {item['reason']}")
    
    # ä¿å­˜
    with open("/tmp/xueqiu_top10.json", "w", encoding="utf-8") as f:
        json.dump(top10, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ° /tmp/xueqiu_top10.json")
