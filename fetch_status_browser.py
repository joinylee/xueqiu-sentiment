#!/usr/bin/env python3
"""
é›ªçƒä¸ªè‚¡è®¨è®ºæŠ“å– - æµè§ˆå™¨ç»•è¿‡WAFç‰ˆ
ä½¿ç”¨ openclaw browser å·¥å…·ç»•è¿‡åçˆ¬
"""

import json
import subprocess
import time
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
import config


def fetch_with_browser(symbol: str, max_id: Optional[int] = None, count: int = 20) -> Dict:
    """
    ä½¿ç”¨ browser å·¥å…·æŠ“å–æ•°æ®
    """
    url = f'https://xueqiu.com/query/v1/symbol/search/status?symbol={symbol}&count={count}&comment=0'
    if max_id:
        url += f'&max_id={max_id}'
    
    try:
        # æ‰“å¼€é¡µé¢
        result = subprocess.run(
            ['openclaw', 'browser', 'open', url],
            capture_output=True, text=True, timeout=30
        )
        
        if result.returncode != 0:
            print(f"   âš ï¸ browser open å¤±è´¥: {result.stderr}")
            return {'list': []}
        
        # è§£æ targetId (æ ¼å¼: "id: xxx")
        import re
        match = re.search(r'id:\s*([A-F0-9]+)', result.stdout)
        if not match:
            print(f"   âš ï¸ æ— æ³•è·å– targetId")
            return {'list': []}
        
        target_id = match.group(1)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(2)
        
        # è·å–é¡µé¢å†…å®¹
        result = subprocess.run(
            ['openclaw', 'browser', 'snapshot', '--target-id', target_id],
            capture_output=True, text=True, timeout=30
        )
        
        if result.returncode != 0:
            print(f"   âš ï¸ snapshot å¤±è´¥: {result.stderr}")
            return {'list': []}
        
        # å…³é—­é¡µé¢
        subprocess.run(
            ['openclaw', 'browser', 'close', '--target-id', target_id],
            capture_output=True, timeout=10
        )
        
        # è§£æJSONæ•°æ®ï¼ˆä»é¡µé¢å†…å®¹ä¸­æå–ï¼‰
        output = result.stdout
        
        # å°è¯•ä»æ–‡æœ¬åŒºåŸŸæå–JSON
        textarea_match = re.search(r'<textarea[^>]*>(.*?)</textarea>', output, re.DOTALL)
        if textarea_match:
            content = textarea_match.group(1)
            try:
                data = json.loads(content)
                return data
            except:
                pass
        
        # å°è¯•ç›´æ¥è§£ææ•´ä¸ªè¾“å‡º
        try:
            # æ‰¾åˆ°JSONå¼€å§‹çš„ä½ç½®
            json_start = output.find('{')
            if json_start >= 0:
                # å°è¯•è§£æ
                data = json.loads(output[json_start:])
                if 'list' in data or 'statuses' in data:
                    return data
        except:
            pass
        
        # æœ€åå°è¯•ï¼šæŸ¥æ‰¾é¡µé¢ä¸­çš„JSONæ•°æ®
        generic_match = re.search(r'generic \[ref=[^\]]+\]: "({.*?})"', output, re.DOTALL)
        if generic_match:
            try:
                # éœ€è¦å¤„ç†è½¬ä¹‰
                json_str = generic_match.group(1).replace('\\"', '"').replace('\\n', '\n')
                data = json.loads(json_str)
                return data
            except:
                pass
        
        print(f"   âš ï¸ æ— æ³•è§£æå“åº”æ•°æ®")
        return {'list': []}
        
    except Exception as e:
        print(f"   âŒ å¼‚å¸¸: {e}")
        return {'list': []}


def fetch_discussions_24h(symbol: str, max_pages: int = 5) -> List[Dict[str, Any]]:
    """
    æŠ“å–24å°æ—¶å†…çš„è®¨è®ºï¼ˆè‡ªåŠ¨ç¿»é¡µï¼‰
    """
    print(f"\nğŸ“¡ æŠ“å– {symbol} çš„24å°æ—¶è®¨è®º...")
    
    now = datetime.now().timestamp() * 1000
    one_day_ms = 24 * 60 * 60 * 1000
    
    all_posts = []
    max_id = None
    page = 1
    
    while page <= max_pages:
        print(f"   ğŸ“„ ç¬¬ {page} é¡µ...")
        
        data = fetch_with_browser(symbol, max_id)
        posts = data.get('list', [])
        
        if not posts:
            print(f"   âœ“ æ— æ›´å¤šæ•°æ®")
            break
        
        # å¤„ç†æœ¬é¡µæ•°æ®
        stop_fetching = False
        valid_count = 0
        for post in posts:
            ts = post.get('created_at', 0)
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡24å°æ—¶
            if now - ts > one_day_ms:
                print(f"   â° è¶…è¿‡24å°æ—¶")
                stop_fetching = True
                break
            
            all_posts.append(normalize_post(post, symbol))
            valid_count += 1
        
        print(f"      æœ¬é¡µæœ‰æ•ˆ: {valid_count}/{len(posts)} æ¡")
        
        if stop_fetching or valid_count < len(posts):
            break
        
        # ä¸‹ä¸€é¡µ
        max_id = posts[-1].get('id')
        page += 1
        
        # é™é€Ÿ
        time.sleep(1.5)
    
    print(f"   âœ… å…± {len(all_posts)} æ¡")
    return all_posts


def normalize_post(post: Dict, symbol: str) -> Dict[str, Any]:
    """æ ‡å‡†åŒ–å•æ¡è®¨è®º"""
    html_text = post.get('text', '')
    plain_text = re.sub(r'<[^>]+>', '', html_text)
    plain_text = plain_text.replace('&nbsp;', ' ').replace('&quot;', '"').strip()
    
    user = post.get('user', {})
    
    return {
        "id": post.get('id'),
        "symbol": symbol,
        "text": plain_text,
        "author": user.get('screen_name', 'åŒ¿å'),
        "author_followers": user.get('followers_count', 0),
        "timestamp": post.get('created_at'),
        "time_str": format_timestamp(post.get('created_at')),
        "likes": post.get('like_count', 0),
        "comments": post.get('reply_count', 0),
        "views": post.get('view_count', 0),
        "source": post.get('source', ''),
    }


def format_timestamp(ts: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³"""
    if not ts:
        return "æœªçŸ¥"
    
    dt = datetime.fromtimestamp(ts / 1000)
    now = datetime.now()
    diff = now - dt
    
    if diff.days == 0:
        hours = diff.seconds // 3600
        if hours == 0:
            minutes = diff.seconds // 60
            return f"{minutes}åˆ†é’Ÿå‰"
        return f"{hours}å°æ—¶å‰"
    elif diff.days == 1:
        return "æ˜¨å¤©"
    else:
        return f"{diff.days}å¤©å‰"


def emotion_analysis(text: str) -> str:
    """ç®€å•æƒ…ç»ªåˆ†æ"""
    bullish = ['æ¶¨', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'çªç ´', 'å¼ºåŠ¿', 'æ–°é«˜', 'åšå¤š', 'æŠ„åº•', 'ä½å¸', 'æ‹‰å‡']
    bearish = ['è·Œ', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'ç ´ä½', 'å¼±åŠ¿', 'æ–°ä½', 'åšç©º', 'å‰²è‚‰', 'é«˜æŠ›', 'è·³æ°´']
    
    text = text.lower()
    bull_count = sum(1 for w in bullish if w in text)
    bear_count = sum(1 for w in bearish if w in text)
    
    if bull_count > bear_count:
        return "åˆ©å¤š"
    elif bear_count > bull_count:
        return "åˆ©ç©º"
    return "ä¸­æ€§"


def batch_fetch(symbols: List[str]) -> Dict[str, List[Dict]]:
    """æ‰¹é‡æŠ“å–å¤šåªè‚¡ç¥¨"""
    results = {}
    
    for symbol in symbols:
        posts = fetch_discussions_24h(symbol)
        results[symbol] = posts
        time.sleep(1)  # è‚¡ç¥¨é—´é™é€Ÿ
    
    return results


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python fetch_status_browser.py <è‚¡ç¥¨ä»£ç >")
        print("ç¤ºä¾‹: python fetch_status_browser.py SH600118")
        sys.exit(1)
    
    symbols = sys.argv[1:]
    
    print("="*60)
    print("ğŸ§ é›ªçƒ24å°æ—¶èˆ†æƒ…æŠ“å– (æµè§ˆå™¨ç‰ˆ)")
    print("="*60)
    
    all_data = batch_fetch(symbols)
    
    # ä¿å­˜
    output = {
        "fetch_time": datetime.now().isoformat(),
        "symbols": symbols,
        "data": all_data
    }
    
    with open('/tmp/xueqiu_24h_browser.json', 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ° /tmp/xueqiu_24h_browser.json")
    
    # ç»Ÿè®¡
    print("\nğŸ“Š æŠ“å–ç»Ÿè®¡:")
    for symbol, posts in all_data.items():
        print(f"\n   {symbol}: {len(posts)} æ¡")
        if posts:
            emotions = {"åˆ©å¤š": 0, "åˆ©ç©º": 0, "ä¸­æ€§": 0}
            for p in posts:
                emotions[emotion_analysis(p['text'])] += 1
            print(f"      æƒ…ç»ª: åˆ©å¤š{emotions['åˆ©å¤š']} åˆ©ç©º{emotions['åˆ©ç©º']} ä¸­æ€§{emotions['ä¸­æ€§']}")
            print(f"      æœ€æ–°: {posts[0]['text'][:50]}...")
