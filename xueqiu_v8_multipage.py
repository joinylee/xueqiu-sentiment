#!/usr/bin/env python3
"""
é›ªçƒèˆ†æƒ…ç›‘æ§ - V8 å¤šé¡µç‰ˆ
æ”¯æŒç¿»é¡µï¼Œè·å–æ›´å¤šè®¨è®ºå†…å®¹
"""

import subprocess
import re
import time
import os
from datetime import datetime

SYMBOLS = [
    ("SH600118", "ä¸­å›½å«æ˜Ÿ"),
    ("SZ002155", "æ¹–å—é»„é‡‘"), 
    ("SZ300456", "èµ›å¾®ç”µå­"),
    ("SH600879", "èˆªå¤©ç”µå­"),
    ("SZ002565", "é¡ºçè‚¡ä»½"),
]

OUTPUT_DIR = "/Users/joinylee/Openclaw/xueqiu_sentiment/reports"
MAX_PAGES = 5  # æœ€å¤§ç¿»é¡µæ•°

def get_sentiment(text):
    bullish = ['æ¶¨', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'çªç ´', 'å¼ºåŠ¿', 'æ–°é«˜', 'åšå¤š', 'æŠ„åº•', 'æ‹‰å‡']
    bearish = ['è·Œ', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'ç ´ä½', 'å¼±åŠ¿', 'æ–°ä½', 'åšç©º', 'å‰²è‚‰', 'æ±ªæ±ª']
    text = text.lower()
    bull = sum(1 for w in bullish if w in text)
    bear = sum(1 for w in bearish if w in text)
    if bull > bear: return "ğŸŸ¢"
    elif bear > bull: return "ğŸ”´"
    return "âšª"

def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    text = text.replace('\\n', '\n').replace('\\t', ' ').replace('\\"', '"')
    text = re.sub(r'<[^>]+>', ' ', text)
    text = re.sub(r'\$[^$]+\$', '', text)
    text = ' '.join(text.split())
    return text.strip()

def extract_posts(raw_text):
    """æå–å¸–å­"""
    posts = []
    text_matches = list(re.finditer(r'"text":"(.*?)"[,}]', raw_text, re.DOTALL))
    time_matches = list(re.finditer(r'"created_at":(\d+)', raw_text))
    author_matches = list(re.finditer(r'"screen_name":"(.*?)"', raw_text))
    
    for i, tm in enumerate(text_matches[:20]):  # æ¯é¡µæœ€å¤š20æ¡
        try:
            text = tm.group(1)
            text = clean_text(text)
            if len(text) < 5 or len(text) > 500:
                continue
            
            ts = int(time_matches[i].group(1)) if i < len(time_matches) else 0
            author = author_matches[i].group(1) if i < len(author_matches) else "åŒ¿å"
            author = author.replace('\\"', '"')
            
            posts.append({
                'text': text[:150],
                'author': author[:20],
                'time': datetime.fromtimestamp(ts/1000).strftime('%m-%d %H:%M') if ts else '',
                'sentiment': get_sentiment(text),
                'timestamp': ts,
            })
        except:
            continue
    
    return posts

def fetch_page(symbol, page=1):
    """æŠ“å–å•é¡µ"""
    ts = int(time.time() * 1000)
    url = f"https://xueqiu.com/statuses/search.json?count=20&symbol={symbol}&page={page}&_={ts}"
    
    try:
        r1 = subprocess.run(['openclaw', 'browser', 'open', url], 
            capture_output=True, text=True, timeout=30)
        
        m = re.search(r'id:\s*([A-F0-9]+)', r1.stdout)
        if not m: return []
        
        tid = m.group(1)
        time.sleep(2)
        
        r2 = subprocess.run(['openclaw', 'browser', 'snapshot', '--target-id', tid],
            capture_output=True, text=True, timeout=30)
        
        subprocess.run(['openclaw', 'browser', 'close', '--target-id', tid],
            capture_output=True, timeout=10)
        
        line = r2.stdout.strip()
        if 'generic [ref=' not in line:
            return []
        
        pos = line.find(': "')
        if pos < 0: return []
        
        start = pos + 2
        end = line.rfind('"')
        if end <= start: return []
        
        raw = line[start:end]
        raw = raw.replace('\\"', '"').replace('\\\\', '\\')
        
        return extract_posts(raw)
        
    except Exception as e:
        return []

def fetch_stock(symbol, name):
    """æŠ“å–å¤šé¡µ"""
    print(f"\nğŸ“ˆ {name} ({symbol})")
    
    all_posts = []
    seen_texts = set()  # å»é‡
    
    for page in range(1, MAX_PAGES + 1):
        print(f"   ç¬¬ {page} é¡µ...", end=" ", flush=True)
        
        posts = fetch_page(symbol, page)
        if not posts:
            print("æ— æ•°æ®")
            break
        
        # å»é‡
        new_posts = []
        for p in posts:
            if p['text'] not in seen_texts:
                seen_texts.add(p['text'])
                new_posts.append(p)
        
        all_posts.extend(new_posts)
        print(f"{len(new_posts)} æ¡")
        
        time.sleep(1.5)
    
    bull = len([p for p in all_posts if p['sentiment'] == 'ğŸŸ¢'])
    bear = len([p for p in all_posts if p['sentiment'] == 'ğŸ”´'])
    
    print(f"   âœ… æ€»è®¡: {len(all_posts)} æ¡ (ğŸŸ¢{bull} ğŸ”´{bear})")
    
    for i, p in enumerate(all_posts[:3], 1):
        print(f"   {i}. {p['sentiment']} {p['text'][:40]}...")
    
    return all_posts

def main():
    print("=" * 60)
    print("ğŸ§ é›ªçƒèˆ†æƒ…ç›‘æ§ - V8 å¤šé¡µç‰ˆ")
    print(f"â° {datetime.now().strftime('%H:%M:%S')}")
    print(f"ğŸ“„ æ¯åªè‚¡ç¥¨æœ€å¤š {MAX_PAGES} é¡µ")
    print("=" * 60)
    
    all_data = {}
    total = 0
    
    for symbol, name in SYMBOLS:
        posts = fetch_stock(symbol, name)
        all_data[symbol] = posts
        total += len(posts)
        time.sleep(1)
    
    # ä¿å­˜
    import json
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    with open(f'{OUTPUT_DIR}/xueqiu_{ts}.json', 'w', encoding='utf-8') as f:
        json.dump({'time': datetime.now().isoformat(), 'data': all_data}, f, ensure_ascii=False, indent=2)
    
    # æ±‡æ€»
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æ€»è®¡: {total} æ¡")
    print("=" * 60)
    for symbol, name in SYMBOLS:
        posts = all_data[symbol]
        bull = len([p for p in posts if p['sentiment'] == 'ğŸŸ¢'])
        bear = len([p for p in posts if p['sentiment'] == 'ğŸ”´'])
        print(f"   {name}: {len(posts)} æ¡ (ğŸŸ¢{bull} ğŸ”´{bear})")

if __name__ == "__main__":
    main()
